{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSML Evaluation Analysis\n",
    "\n",
    "This notebook analyzes the SSML prediction results saved as JSON files from the model.py script.\n",
    "\n",
    "It includes:\n",
    "1. Visualizations of tag usage metrics\n",
    "2. Error metrics analysis (MAE/MSE for prosody parameters)\n",
    "3. Break precision/recall/F1 analysis\n",
    "4. Model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set cuda visible devices to 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Any, Optional\n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_dir = \"visualizations\"\n",
    "os.makedirs(viz_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(results_dir=\"results\", model_name=None):\n",
    "    \"\"\"Load results from the specified directory\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Update the search pattern to match the new file naming convention\n",
    "    if model_name:\n",
    "        # Look for files like \"model_name_zero_shot.json\" or \"model_name_few_shot_*.json\"\n",
    "        search_path = os.path.join(results_dir, f\"{model_name}_*.json\")\n",
    "    else:\n",
    "        search_path = os.path.join(results_dir, \"*.json\")\n",
    "    \n",
    "    # Find all json files matching the pattern\n",
    "    json_files = glob.glob(search_path, recursive=True)\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                file_name = os.path.basename(file_path)\n",
    "                results[file_name] = json.load(f)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results for a specific model\n",
    "model_name = \"mistral\"  # Change to the model you want to analyze\n",
    "results = load_results(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Metrics Visualization\n",
    "\n",
    "Visualize tag counts, prosody parameters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def compute_r2(evaluation_results: dict, key: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute R² for a given parameter key comparing predictions to ground truth.\n",
    "    \"\"\"\n",
    "    metrics = evaluation_results.get(\"metrics\", {})\n",
    "    \n",
    "    # Get true and predicted values based on the key\n",
    "    if key == \"break_time\":\n",
    "        y_true = metrics.get(\"true_break_time_values\", [])\n",
    "        y_pred = metrics.get(\"pred_break_time_values\", [])\n",
    "    elif key == \"pitch\":\n",
    "        y_true = metrics.get(\"true_pitch_values\", [])\n",
    "        y_pred = metrics.get(\"pred_pitch_values\", [])\n",
    "    elif key == \"rate\":\n",
    "        y_true = metrics.get(\"true_rate_values\", [])\n",
    "        y_pred = metrics.get(\"pred_rate_values\", [])\n",
    "    elif key == \"volume\":\n",
    "        y_true = metrics.get(\"true_volume_values\", [])\n",
    "        y_pred = metrics.get(\"pred_volume_values\", [])\n",
    "    else:\n",
    "        print(f\"Unknown key: {key}\")\n",
    "        return float('nan')\n",
    "    \n",
    "    print(f\"R² calculation for {key}: {len(y_true)} true values, {len(y_pred)} predicted values\")\n",
    "    \n",
    "    # Now both arrays should be the same length (paired data)\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(f\"ERROR: Mismatched lengths for {key}: true={len(y_true)}, pred={len(y_pred)}\")\n",
    "        return float('nan')\n",
    "    \n",
    "    if len(y_true) < 2:\n",
    "        print(f\"Not enough paired data for {key}: {len(y_true)} pairs\")\n",
    "        return float('nan')\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Check if there's any variance in true values\n",
    "    var_true = np.var(y_true)\n",
    "    if var_true == 0:\n",
    "        print(f\"No variance in true {key} values (all values are the same)\")\n",
    "        return float('nan')\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"True values range: {np.min(y_true):.2f} to {np.max(y_true):.2f}, var={var_true:.3f}\")\n",
    "    print(f\"Predicted values range: {np.min(y_pred):.2f} to {np.max(y_pred):.2f}, var={np.var(y_pred):.3f}\")\n",
    "    \n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\"R² for {key}: {r2:.3f}\")\n",
    "        \n",
    "        # Warn if R² is very negative\n",
    "        if r2 < -1:\n",
    "            print(f\"WARNING: Very negative R² ({r2:.3f}) suggests poor model performance\")\n",
    "        \n",
    "        return r2\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating R² for {key}: {e}\")\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(evaluation_results: Dict[str, Any], save_path: Optional[str] = None):\n",
    "    \"\"\"Visualize tag usage and prosody metrics.\"\"\"\n",
    "    metrics = evaluation_results.get(\"metrics\", {})\n",
    "    name = evaluation_results.get(\"model_name\", \"model\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Tag counts\n",
    "    plt.subplot(2, 2, 1)\n",
    "    counts = {\n",
    "        \"Total Tags\": metrics.get(\"total_tags_mean\", 0),\n",
    "        \"Prosody Tags\": metrics.get(\"prosody_count_mean\", 0),\n",
    "        \"Break Tags\": metrics.get(\"break_count_mean\", 0)\n",
    "    }\n",
    "    plt.bar(counts.keys(), counts.values())\n",
    "    plt.title(f\"Tag Distribution - {name}\")\n",
    "    plt.ylabel(\"Avg Count\")\n",
    "\n",
    "    # Prosody means\n",
    "    plt.subplot(2, 2, 2)\n",
    "    pros_means = {\n",
    "        \"Pitch\": metrics.get(\"pitch_adjustments_mean_mean\", 0),\n",
    "        \"Rate\": metrics.get(\"rate_adjustments_mean_mean\", 0),\n",
    "        \"Volume\": metrics.get(\"volume_adjustments_mean_mean\", 0)\n",
    "    }\n",
    "    plt.bar(pros_means.keys(), pros_means.values())\n",
    "    plt.title(f\"Avg Prosody Parameters - {name}\")\n",
    "    plt.ylabel(\"Mean (%)\")\n",
    "\n",
    "    # Prosody variability\n",
    "    plt.subplot(2, 2, 3)\n",
    "    pros_vars = {\n",
    "        \"Pitch\": metrics.get(\"pitch_adjustments_std_mean\", 0),\n",
    "        \"Rate\": metrics.get(\"rate_adjustments_std_mean\", 0),\n",
    "        \"Volume\": metrics.get(\"volume_adjustments_std_mean\", 0)\n",
    "    }\n",
    "    plt.bar(pros_vars.keys(), pros_vars.values())\n",
    "    plt.title(f\"Prosody Variability - {name}\")\n",
    "    plt.ylabel(\"Std Dev (%)\")\n",
    "\n",
    "    # Tags per sentence\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar([\"Tags/Sent\"], [metrics.get(\"tags_per_sentence_mean\", 0)])\n",
    "    plt.title(f\"Tag Density - {name}\")\n",
    "    plt.ylabel(\"Avg Tags/Sentence\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics Visualization\n",
    "\n",
    "Visualize MAE/MSE and Precision/Recall/F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_and_r2(evaluation_results: Dict[str, Any], save_path: Optional[str] = None):\n",
    "    \"\"\"Plot MAE, MSE, and R² for prosody and break_time.\"\"\"\n",
    "    m = evaluation_results.get(\"metrics\", {})\n",
    "    params = [\"pitch\", \"rate\", \"volume\", \"break_time\"]\n",
    "\n",
    "    labels, mae_vals, mse_vals, r2_vals = [], [], [], []\n",
    "    for p in params:\n",
    "        mae_k, mse_k = f\"{p}_mae\", f\"{p}_mse\"\n",
    "        if mae_k in m and mse_k in m:\n",
    "            labels.append(p.capitalize())\n",
    "            mae_vals.append(m[mae_k])\n",
    "            mse_vals.append(m[mse_k])\n",
    "            # Call compute_r2 with the evaluation_results dict, not a list\n",
    "            r2_val = compute_r2(evaluation_results, p)\n",
    "            r2_vals.append(r2_val)\n",
    "            print(f\"Added R² value for {p}: {r2_val}\")\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.25\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "    # MAE/MSE\n",
    "    ax1 = axes[0]\n",
    "    ax1.bar(x - width/2, mae_vals, width, label=\"MAE\", alpha=0.7)\n",
    "    ax1.bar(x + width/2, mse_vals, width, label=\"MSE\", alpha=0.7)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.set_title(f\"MAE vs MSE - {evaluation_results.get('model_name', '')}\")\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Precision/Recall/F1 if present\n",
    "    if all(k in m for k in [\"break_precision\", \"break_recall\", \"break_f1\"]):\n",
    "        ax2 = axes[1]\n",
    "        prf_labels = [\"Breaks\"]\n",
    "        prf_vals = [m[\"break_precision\"], m[\"break_recall\"], m[\"break_f1\"]]\n",
    "        x2 = np.arange(len(prf_labels))\n",
    "        w2 = width\n",
    "        ax2.bar(x2 - w2, [prf_vals[0]], w2, label=\"Precision\", alpha=0.7)\n",
    "        ax2.bar(x2, [prf_vals[1]], w2, label=\"Recall\", alpha=0.7)\n",
    "        ax2.bar(x2 + w2, [prf_vals[2]], w2, label=\"F1\", alpha=0.7)\n",
    "        ax2.set_xticks(x2)\n",
    "        ax2.set_xticklabels(prf_labels)\n",
    "        ax2.set_title(f\"Break P/R/F1 - {evaluation_results.get('model_name', '')}\")\n",
    "        ax2.set_ylim(0,1.1)\n",
    "        ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax2.legend()\n",
    "\n",
    "    # R²\n",
    "    ax3 = axes[-1]\n",
    "    # Convert NaN values to 0 for display purposes\n",
    "    r2_display_vals = []\n",
    "    for val in r2_vals:\n",
    "        if np.isnan(val):\n",
    "            r2_display_vals.append(0)\n",
    "        elif val < -1:  # For very negative R² values, clamp to -1 for visualization\n",
    "            r2_display_vals.append(-1)\n",
    "        else:\n",
    "            r2_display_vals.append(val)\n",
    "    \n",
    "    print(f\"Original R² values: {r2_vals}\")\n",
    "    print(f\"R² display values: {r2_display_vals}\")\n",
    "    \n",
    "    # Create the bars with clamped values for display\n",
    "    bars = ax3.bar(x, r2_display_vals, width, label=\"R²\", alpha=0.7)\n",
    "    \n",
    "    # Set colors based on R² value\n",
    "    for i, bar in enumerate(bars):\n",
    "        if not np.isnan(r2_vals[i]):\n",
    "            if r2_vals[i] < 0:\n",
    "                bar.set_color('salmon')  # Negative R² in red\n",
    "            else:\n",
    "                bar.set_color('skyblue')  # Positive R² in blue\n",
    "    \n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(labels)\n",
    "    ax3.set_title(f\"Coefficient of Determination (R²) - {evaluation_results.get('model_name', '')}\")\n",
    "    ax3.set_ylim(-1.1, 1.1)  # R² can be negative\n",
    "    ax3.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)  # Add zero line\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Add value labels on bars - use original values for the text\n",
    "    for i, (bar, val) in enumerate(zip(bars, r2_vals)):\n",
    "        if not np.isnan(val):\n",
    "            # Format text based on the magnitude of R²\n",
    "            if val < -10:\n",
    "                label_text = f\"{val:.1f}\"  # Shorter format for very negative values\n",
    "            else:\n",
    "                label_text = f\"{val:.2f}\"\n",
    "            \n",
    "            # Position text based on bar height and value\n",
    "            display_val = r2_display_vals[i]\n",
    "            if display_val >= 0:\n",
    "                y_pos = display_val + 0.1\n",
    "                va = 'bottom'\n",
    "            else:\n",
    "                # For negative bars, position text above the bar\n",
    "                y_pos = -0.05\n",
    "                va = 'bottom'\n",
    "            \n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, y_pos, \n",
    "                    label_text, ha='center', va=va, \n",
    "                    color='black', fontweight='bold', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_model_results(models=None, results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Load results from multiple model files with the new naming pattern\n",
    "    \n",
    "    Args:\n",
    "        models: List of model names to load (None for all model names in filenames)\n",
    "        results_dir: Base results directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of results by model name and result type\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    # If no models specified, extract model names from filenames\n",
    "    if models is None:\n",
    "        models = set()\n",
    "        for item in os.listdir(results_dir):\n",
    "            if item.endswith('.json'):\n",
    "                # Extract model name from file patterns like \"mistral_zero_shot.json\"\n",
    "                parts = item.split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    models.add(parts[0])\n",
    "    \n",
    "    print(f\"Loading results for models: {models}\")\n",
    "    \n",
    "    for model in models:\n",
    "        # Find all files for this model\n",
    "        model_results = load_results(results_dir=results_dir, model_name=model)\n",
    "        all_results[model] = model_results\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple SSML similarity using sentence transformers\n",
    "def calculate_ssml_similarity(all_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Calculate and plot SSML cosine similarity using sentence transformers.\n",
    "    Simple approach: just encode the SSML strings directly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"Using sentence transformers for SSML similarity\")\n",
    "    except ImportError:\n",
    "        print(\"sentence-transformers not available, skipping SSML similarity analysis\")\n",
    "        return {}\n",
    "    \n",
    "    model_similarities = {}\n",
    "    \n",
    "    for model_name, results in all_results.items():\n",
    "        model_similarities[model_name] = {}\n",
    "        \n",
    "        for file_name, result_data in results.items():\n",
    "            # Extract approach (zero_shot, few_shot, etc.)\n",
    "            if \"_zero_shot.json\" in file_name:\n",
    "                approach = \"zero_shot\"\n",
    "            elif \"_few_shot.json\" in file_name:\n",
    "                approach = \"few_shot\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Get SSML pairs\n",
    "            if \"results\" not in result_data:\n",
    "                continue\n",
    "                \n",
    "            pred_ssmls = []\n",
    "            gold_ssmls = []\n",
    "            \n",
    "            for result in result_data[\"results\"]:\n",
    "                pred_ssml = result.get(\"predicted_ssml\") or result.get(\"ssml\", \"\")\n",
    "                gold_ssml = result.get(\"gold_ssml\", \"\")\n",
    "                \n",
    "                if pred_ssml.strip() and gold_ssml.strip():\n",
    "                    pred_ssmls.append(pred_ssml)\n",
    "                    gold_ssmls.append(gold_ssml)\n",
    "            \n",
    "            if not pred_ssmls:\n",
    "                print(f\"No valid SSML pairs found for {model_name} {approach}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate embeddings - just encode the SSML directly\n",
    "            pred_embeddings = model.encode(pred_ssmls)\n",
    "            gold_embeddings = model.encode(gold_ssmls)\n",
    "            \n",
    "            # Calculate pairwise cosine similarities\n",
    "            similarities = []\n",
    "            for pred_emb, gold_emb in zip(pred_embeddings, gold_embeddings):\n",
    "                similarity = cosine_similarity([pred_emb], [gold_emb])[0, 0]\n",
    "                similarities.append(similarity)\n",
    "            \n",
    "            model_similarities[model_name][approach] = {\n",
    "                \"cosine_similarities\": similarities,\n",
    "                \"mean_similarity\": np.mean(similarities),\n",
    "                \"std_similarity\": np.std(similarities),\n",
    "                \"n_pairs\": len(similarities)\n",
    "            }\n",
    "            \n",
    "            print(f\"{model_name} {approach}: {np.mean(similarities):.3f} ± {np.std(similarities):.3f} (n={len(similarities)})\")\n",
    "    \n",
    "    return model_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your compare_models function to actually plot SSML similarities\n",
    "def compare_models(comparison_results: Dict[str, Any], save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Plot comparison of multiple models including SSML cosine similarity.\n",
    "    \"\"\"\n",
    "    evaluations = comparison_results[\"evaluations\"]\n",
    "    model_names = [eval_result[\"model_name\"] for eval_result in evaluations]\n",
    "    num_models = len(model_names)\n",
    "    x = np.arange(num_models)\n",
    "    width = 0.7\n",
    "\n",
    "    # Extract all existing metrics (same as before)\n",
    "    total_tags = [e[\"metrics\"].get(\"total_tags_mean\", 0) for e in evaluations]\n",
    "    prosody_tags = [e[\"metrics\"].get(\"prosody_count_mean\", 0) for e in evaluations]\n",
    "    break_tags = [e[\"metrics\"].get(\"break_count_mean\", 0) for e in evaluations]\n",
    "    \n",
    "    gold_total_tags = [e[\"metrics\"].get(\"gold_total_tags_mean\", 0) for e in evaluations]\n",
    "    gold_prosody_tags = [e[\"metrics\"].get(\"gold_prosody_count_mean\", 0) for e in evaluations]\n",
    "    gold_break_tags = [e[\"metrics\"].get(\"gold_break_count_mean\", 0) for e in evaluations]\n",
    "    \n",
    "    pitch_mae = [e[\"metrics\"].get(\"pitch_mae\", 0) for e in evaluations]\n",
    "    rate_mae = [e[\"metrics\"].get(\"rate_mae\", 0) for e in evaluations]\n",
    "    break_time_mae = [e[\"metrics\"].get(\"break_time_mae\", 0) for e in evaluations]\n",
    "    volume_mae = [e[\"metrics\"].get(\"volume_mae\", 0) for e in evaluations]\n",
    "    \n",
    "    pitch_mse = [e[\"metrics\"].get(\"pitch_mse\", 0) for e in evaluations]\n",
    "    rate_mse = [e[\"metrics\"].get(\"rate_mse\", 0) for e in evaluations]\n",
    "    break_time_mse = [e[\"metrics\"].get(\"break_time_mse\", 0) for e in evaluations]\n",
    "    volume_mse = [e[\"metrics\"].get(\"volume_mse\", 0) for e in evaluations]\n",
    "    \n",
    "    # Compute RMSE from MSE\n",
    "    pitch_rmse = np.sqrt(pitch_mse)\n",
    "    rate_rmse = np.sqrt(rate_mse)\n",
    "    break_time_rmse = np.sqrt(break_time_mse)\n",
    "    volume_rmse = np.sqrt(volume_mse)\n",
    "    \n",
    "    # Calculate R² values for each model\n",
    "    pitch_r2 = []\n",
    "    rate_r2 = []\n",
    "    break_time_r2 = []\n",
    "    volume_r2 = []\n",
    "    for eval_result in evaluations:\n",
    "        pitch_r2.append(compute_r2(eval_result, \"pitch\"))\n",
    "        rate_r2.append(compute_r2(eval_result, \"rate\"))\n",
    "        break_time_r2.append(compute_r2(eval_result, \"break_time\"))\n",
    "        volume_r2.append(compute_r2(eval_result, \"volume\"))\n",
    "    \n",
    "    precision = [e[\"metrics\"].get(\"break_precision\", 0) for e in evaluations]\n",
    "    recall = [e[\"metrics\"].get(\"break_recall\", 0) for e in evaluations]\n",
    "    f1 = [e[\"metrics\"].get(\"break_f1\", 0) for e in evaluations]\n",
    "    \n",
    "    tags_per_sent = [e[\"metrics\"].get(\"tags_per_sentence_mean\", 0) for e in evaluations]\n",
    "\n",
    "    # Calculate SSML similarities for these evaluations\n",
    "    ssml_similarities = []\n",
    "    for eval_result in tqdm(evaluations, desc=\"Calculating SSML similarities\"):\n",
    "        if \"results\" not in eval_result:\n",
    "            ssml_similarities.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            \n",
    "            pred_ssmls = []\n",
    "            gold_ssmls = []\n",
    "            \n",
    "            for result in eval_result[\"results\"]:\n",
    "                pred_ssml = result.get(\"predicted_ssml\") or result.get(\"ssml\", \"\")\n",
    "                gold_ssml = result.get(\"gold_ssml\", \"\")\n",
    "                \n",
    "                if pred_ssml.strip() and gold_ssml.strip():\n",
    "                    pred_ssmls.append(pred_ssml)\n",
    "                    gold_ssmls.append(gold_ssml)\n",
    "            \n",
    "            if pred_ssmls:\n",
    "                pred_embeddings = model.encode(pred_ssmls)\n",
    "                gold_embeddings = model.encode(gold_ssmls)\n",
    "                \n",
    "                similarities = []\n",
    "                for pred_emb, gold_emb in zip(pred_embeddings, gold_embeddings):\n",
    "                    similarity = cosine_similarity([pred_emb], [gold_emb])[0, 0]\n",
    "                    similarities.append(similarity)\n",
    "                \n",
    "                ssml_similarities.append(np.mean(similarities))\n",
    "                print(f\"SSML similarity for {eval_result.get('model_name', 'unknown')}: {np.mean(similarities):.3f}\")\n",
    "            else:\n",
    "                ssml_similarities.append(0.0)\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"sentence-transformers not available, skipping SSML similarity\")\n",
    "            ssml_similarities.append(0.0)\n",
    "\n",
    "    # Create a 6x3 grid of subplots (keep your existing structure)\n",
    "    fig, axes = plt.subplots(6, 3, figsize=(22, 32))\n",
    "    plt.subplots_adjust(bottom=0.15, hspace=0.4, wspace=0.3)\n",
    "\n",
    "    # Helper for x-tick labels\n",
    "    def set_xticks_labels(ax):\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names, rotation=30, ha='right', fontsize=11)\n",
    "\n",
    "    # Row 1: Tag metrics (same as before)\n",
    "    for i, (ax, vals, gold_vals, title) in enumerate(zip(axes[0], \n",
    "                                  [total_tags, prosody_tags, break_tags],\n",
    "                                  [gold_total_tags, gold_prosody_tags, gold_break_tags],\n",
    "                                  [\"Total Tags\", \"Prosody Tags\", \"Break Tags\"])):\n",
    "        width_bar = 0.35\n",
    "        ax.bar(x - width_bar/2, gold_vals, width_bar, color='goldenrod', label='Gold Standard')\n",
    "        ax.bar(x + width_bar/2, vals, width_bar, color='skyblue', label='Prediction')\n",
    "        ax.set_title(title)\n",
    "        set_xticks_labels(ax)\n",
    "        ax.set_ylabel('Count')\n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "\n",
    "    # Row 2: MAE metrics (same as before)\n",
    "    for ax, vals, title in zip(axes[1], [pitch_mae, rate_mae, break_time_mae],\n",
    "                               [\"Pitch MAE\", \"Rate MAE\", \"Break Time MAE\"]):\n",
    "        ax.bar(x, vals, color='lightgreen')\n",
    "        ax.set_title(title)\n",
    "        set_xticks_labels(ax)\n",
    "        ax.set_ylabel('MAE')\n",
    "\n",
    "    # ADD SSML SIMILARITY to the fourth position in row 2\n",
    "    if any(s > 0 for s in ssml_similarities):  # Only plot if we have valid similarities\n",
    "        ax_sim = axes[1, 2]  # Fourth position in row 2 (0-indexed, so position 2)\n",
    "        bars = ax_sim.bar(x, ssml_similarities, color='mediumpurple')\n",
    "        ax_sim.set_title('SSML Cosine Similarity')\n",
    "        set_xticks_labels(ax_sim)\n",
    "        ax_sim.set_ylabel('Similarity')\n",
    "        ax_sim.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, similarity in zip(bars, ssml_similarities):\n",
    "            if similarity > 0:\n",
    "                ax_sim.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                           f'{similarity:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Row 3: MSE metrics (same as before)\n",
    "    for ax, vals, title in zip(axes[2], [pitch_mse, rate_mse, break_time_mse],\n",
    "                               [\"Pitch MSE\", \"Rate MSE\", \"Break Time MSE\"]):\n",
    "        ax.bar(x, vals, color='orange')\n",
    "        ax.set_title(title)\n",
    "        set_xticks_labels(ax)\n",
    "        ax.set_ylabel('MSE')\n",
    "\n",
    "    # Row 4: RMSE metrics (same as before)\n",
    "    for ax, vals, title in zip(axes[3], [pitch_rmse, rate_rmse, break_time_rmse],\n",
    "                               [\"Pitch RMSE\", \"Rate RMSE\", \"Break Time RMSE\"]):\n",
    "        ax.bar(x, vals, color='violet')\n",
    "        ax.set_title(title)\n",
    "        set_xticks_labels(ax)\n",
    "        ax.set_ylabel('RMSE')\n",
    "\n",
    "    # Row 5: R² metrics (same as before)\n",
    "    for ax, vals, title in zip(axes[4], [pitch_r2, rate_r2, break_time_r2],\n",
    "                               [\"Pitch R²\", \"Rate R²\", \"Break Time R²\"]):\n",
    "        ax.bar(x, vals, color='dodgerblue')\n",
    "        ax.set_title(title)\n",
    "        set_xticks_labels(ax)\n",
    "        ax.set_ylabel('R²')\n",
    "        ax.set_ylim(-1.1, 1.05)\n",
    "        ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Row 6: Break P/R/F1, Tags/Sentence, Volume MAE (same as before)\n",
    "    ax_prf, ax_tps, ax_vol = axes[5]\n",
    "    \n",
    "    # Break Precision/Recall/F1\n",
    "    width_prf = 0.25\n",
    "    ax_prf.bar(x - width_prf, precision, width_prf, label=\"Precision\", alpha=0.7, color='royalblue')\n",
    "    ax_prf.bar(x, recall, width_prf, label=\"Recall\", alpha=0.7, color='lightblue')\n",
    "    ax_prf.bar(x + width_prf, f1, width_prf, label=\"F1\", alpha=0.7, color='navy')\n",
    "    ax_prf.set_title(\"Break Precision/Recall/F1\")\n",
    "    set_xticks_labels(ax_prf)\n",
    "    ax_prf.set_ylim(0, 1.1)\n",
    "    ax_prf.legend()\n",
    "    ax_prf.set_ylabel('Score')\n",
    "\n",
    "    # Tags per sentence\n",
    "    ax_tps.bar(x, tags_per_sent, color='teal')\n",
    "    ax_tps.set_title(\"Tags per Sentence\")\n",
    "    set_xticks_labels(ax_tps)\n",
    "    ax_tps.set_ylabel('Tags/Sentence')\n",
    "\n",
    "    # Volume MAE\n",
    "    ax_vol.bar(x, volume_mae, color='coral')\n",
    "    ax_vol.set_title(\"Volume MAE\")\n",
    "    set_xticks_labels(ax_vol)\n",
    "    ax_vol.set_ylabel('MAE')\n",
    "\n",
    "    fig.suptitle('Model Comparison Metrics (including SSML Similarity)', fontsize=18, fontweight='bold')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_architectures(all_results, mode=\"zero_shot\", save_path=None):\n",
    "    \"\"\"\n",
    "    Compare results across different model architectures with new file naming\n",
    "    \n",
    "    Args:\n",
    "        all_results: Dictionary of results loaded with load_all_model_results\n",
    "        mode: Either \"zero_shot\", \"few_shot\", or \"both\"\n",
    "        save_path: Optional path to save the comparison plot\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    model_mappings = []  # Store which model each evaluation belongs to\n",
    "    \n",
    "    # Process results based on mode\n",
    "    if mode in [\"zero_shot\", \"both\"]:\n",
    "        # Extract zero-shot results from each model\n",
    "        for model, results in all_results.items():\n",
    "            # Find files like \"model_zero_shot.json\"\n",
    "            zero_shot_file = next((f for f in results if f == f\"{model}_zero_shot.json\"), None)\n",
    "            if zero_shot_file:\n",
    "                evaluations.append(results[zero_shot_file])\n",
    "                model_mappings.append((model, \"zero_shot\"))\n",
    "                print(f\"Added zero-shot results for {model}\")\n",
    "    \n",
    "    if mode in [\"few_shot\", \"both\"]:\n",
    "        # Extract few-shot results from each model\n",
    "        for model, results in all_results.items():\n",
    "            # Changed from model_few_shot_all.json to model_few_shot.json\n",
    "            few_shot_file = next((f for f in results if f == f\"{model}_few_shot.json\"), None)\n",
    "            if few_shot_file:\n",
    "                evaluations.append(results[few_shot_file])\n",
    "                model_mappings.append((model, \"few_shot\"))\n",
    "                print(f\"Added few-shot results for {model}\")\n",
    "    \n",
    "    if not evaluations:\n",
    "        print(\"No matching results found to compare\")\n",
    "        return\n",
    "    \n",
    "    # Customize model names for better display\n",
    "    for i, eval_result in enumerate(evaluations):\n",
    "        if i < len(model_mappings):\n",
    "            model_name, approach = model_mappings[i]\n",
    "            # Set a clear model name that includes both architecture and approach\n",
    "            eval_result[\"model_name\"] = f\"{approach.replace('_', '-').title()}-{model_name.capitalize()}\"\n",
    "    \n",
    "    # Create comparison object and plot\n",
    "    comparison = {\"evaluations\": evaluations}\n",
    "    compare_models(comparison, save_path=save_path)\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_grouped(all_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Compare models with zero-shot and few-shot grouped together and sorted by performance.\n",
    "    \n",
    "    Args:\n",
    "        all_results: Dictionary of results loaded with load_all_model_results\n",
    "        save_path: Optional path to save the comparison plot\n",
    "    \"\"\"\n",
    "    # Extract model names and prepare data structure\n",
    "    models = list(all_results.keys())\n",
    "    model_data = {}\n",
    "    \n",
    "    for model in models:\n",
    "        results = all_results[model]\n",
    "        model_data[model] = {\n",
    "            'zero_shot': None,\n",
    "            'few_shot': None\n",
    "        }\n",
    "        \n",
    "        # Get zero-shot and few-shot results if available\n",
    "        zero_shot_file = next((f for f in results if f == f\"{model}_zero_shot.json\"), None)\n",
    "        few_shot_file = next((f for f in results if f == f\"{model}_few_shot.json\"), None)\n",
    "        \n",
    "        if zero_shot_file:\n",
    "            model_data[model]['zero_shot'] = results[zero_shot_file]\n",
    "        if few_shot_file:\n",
    "            model_data[model]['few_shot'] = results[few_shot_file]\n",
    "    \n",
    "    # Create plots with grouped bars\n",
    "    metrics_to_plot = [\n",
    "        ('pitch_mae', 'Pitch MAE', True),  # (metric_key, title, lower_is_better)\n",
    "        ('rate_mae', 'Rate MAE', True),\n",
    "        ('break_time_mae', 'Break Time MAE', True),\n",
    "        ('pitch_mse', 'Pitch MSE', True),\n",
    "        ('rate_mse', 'Rate MSE', True),\n",
    "        ('break_time_mse', 'Break Time MSE', True),\n",
    "        ('total_tags_mean', 'Total Tags', False),\n",
    "        ('prosody_count_mean', 'Prosody Tags', False),\n",
    "        ('break_count_mean', 'Break Tags', False)\n",
    "    ]\n",
    "    \n",
    "    # Create a 3x3 grid of subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # For each metric, create a grouped bar chart\n",
    "    for i, (metric_key, title, lower_is_better) in enumerate(metrics_to_plot):\n",
    "        # Prepare data for this metric\n",
    "        model_values = []\n",
    "        for model in models:\n",
    "            zero_shot_val = None\n",
    "            few_shot_val = None\n",
    "            \n",
    "            if model_data[model]['zero_shot']:\n",
    "                zero_shot_val = model_data[model]['zero_shot']['metrics'].get(metric_key, 0)\n",
    "            if model_data[model]['few_shot']:\n",
    "                few_shot_val = model_data[model]['few_shot']['metrics'].get(metric_key, 0)\n",
    "            \n",
    "            # Only include models that have at least one value\n",
    "            if zero_shot_val is not None or few_shot_val is not None:\n",
    "                # Use zero_shot_val for sorting if available, otherwise few_shot_val\n",
    "                sort_val = zero_shot_val if zero_shot_val is not None else few_shot_val\n",
    "                model_values.append((model, zero_shot_val, few_shot_val, sort_val))\n",
    "        \n",
    "        # Sort models by performance (if there's data to sort)\n",
    "        if model_values:\n",
    "            if lower_is_better:\n",
    "                model_values.sort(key=lambda x: x[3])\n",
    "            else:\n",
    "                model_values.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        # Prepare plotting data\n",
    "        sorted_models = [m[0] for m in model_values]\n",
    "        zero_shot_values = [m[1] if m[1] is not None else 0 for m in model_values]\n",
    "        few_shot_values = [m[2] if m[2] is not None else 0 for m in model_values]\n",
    "        \n",
    "        # Plot on the corresponding subplot\n",
    "        ax = axes[i]\n",
    "        x = np.arange(len(sorted_models))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Create the grouped bars\n",
    "        ax.bar(x - width/2, zero_shot_values, width, label='Zero-Shot', color='skyblue')\n",
    "        ax.bar(x + width/2, few_shot_values, width, label='Few-Shot', color='lightcoral')\n",
    "        \n",
    "        # Add labels and formatting\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(sorted_models, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Create separate plot for P/R/F1 (already grouped)\n",
    "    fig2, ax_prf = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare P/R/F1 data (similar to above but with 3 metrics per model)\n",
    "    prf_data = []\n",
    "    for model in models:\n",
    "        # Initialize with 0 instead of None\n",
    "        zero_prec = zero_rec = zero_f1 = 0\n",
    "        few_prec = few_rec = few_f1 = 0\n",
    "        \n",
    "        if model_data[model]['zero_shot']:\n",
    "            m = model_data[model]['zero_shot']['metrics']\n",
    "            zero_prec = m.get('break_precision', 0) or 0  # Use 0 if None\n",
    "            zero_rec = m.get('break_recall', 0) or 0\n",
    "            zero_f1 = m.get('break_f1', 0) or 0\n",
    "        \n",
    "        if model_data[model]['few_shot']:\n",
    "            m = model_data[model]['few_shot']['metrics']\n",
    "            few_prec = m.get('break_precision', 0) or 0\n",
    "            few_rec = m.get('break_recall', 0) or 0\n",
    "            few_f1 = m.get('break_f1', 0) or 0\n",
    "        \n",
    "        # Sort by F1 score (use zero-shot or few-shot, whichever is bigger)\n",
    "        sort_val = max(zero_f1, few_f1) \n",
    "        prf_data.append((model, zero_prec, zero_rec, zero_f1, few_prec, few_rec, few_f1, sort_val))\n",
    "    \n",
    "    # Sort by F1 score, highest first\n",
    "    prf_data.sort(key=lambda x: x[7], reverse=True)\n",
    "    \n",
    "    # Only create plots if we have data\n",
    "    if prf_data:\n",
    "        # Plot P/R/F1 chart\n",
    "        prf_models = [p[0] for p in prf_data]\n",
    "        x = np.arange(len(prf_models))\n",
    "        width = 0.15\n",
    "        \n",
    "        # Make sure all values are numeric (not None)\n",
    "        # Zero-shot bars\n",
    "        ax_prf.bar(x - width*1.5, [p[1] or 0 for p in prf_data], width, label='Zero-Shot Precision', color='royalblue')\n",
    "        ax_prf.bar(x - width*0.5, [p[2] or 0 for p in prf_data], width, label='Zero-Shot Recall', color='lightblue')\n",
    "        ax_prf.bar(x + width*0.5, [p[3] or 0 for p in prf_data], width, label='Zero-Shot F1', color='darkblue')\n",
    "        \n",
    "        # Few-shot bars\n",
    "        ax_prf.bar(x + width*1.5, [p[4] or 0 for p in prf_data], width, label='Few-Shot Precision', color='darkred')\n",
    "        ax_prf.bar(x + width*2.5, [p[5] or 0 for p in prf_data], width, label='Few-Shot Recall', color='lightcoral')\n",
    "        ax_prf.bar(x + width*3.5, [p[6] or 0 for p in prf_data], width, label='Few-Shot F1', color='maroon')\n",
    "        \n",
    "        ax_prf.set_title('Break Precision/Recall/F1 by Model')\n",
    "        ax_prf.set_xticks(x + width)\n",
    "        ax_prf.set_xticklabels(prf_models, rotation=45, ha='right')\n",
    "        ax_prf.set_ylim(0, 1.1)\n",
    "        ax_prf.legend()\n",
    "        ax_prf.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save both figures if path provided\n",
    "    if save_path:\n",
    "        fig.savefig(f\"{save_path}_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "        if prf_data:  # Only save if we have P/R/F1 data\n",
    "            fig2.savefig(f\"{save_path}_prf.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        if prf_data:\n",
    "            plt.close(fig2)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Specific Results\n",
    "\n",
    "Let's analyze some specific result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze zero-shot results if available\n",
    "zero_shot_file = next((f for f in results if f.startswith(\"zero_shot\")), None)\n",
    "if zero_shot_file:\n",
    "    print(f\"Analyzing zero-shot results from {zero_shot_file}\")\n",
    "    zero_shot_results = results[zero_shot_file]\n",
    "    \n",
    "    # Plot basic metrics\n",
    "    plot_metrics(zero_shot_results)\n",
    "    \n",
    "    # Plot error metrics\n",
    "    plot_error_and_r2(zero_shot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze experiment results\n",
    "if \"experiment_results.json\" in results:\n",
    "    experiment_results = results[\"experiment_results.json\"]\n",
    "    \n",
    "    # Check if there are few-shot voice-specific results\n",
    "    if \"few_shot\" in experiment_results and \"by_voice\" in experiment_results[\"few_shot\"]:\n",
    "        for voice, voice_results in experiment_results[\"few_shot\"][\"by_voice\"].items():\n",
    "            print(f\"Analyzing few-shot results for voice: {voice}\")\n",
    "            \n",
    "            # Plot basic metrics\n",
    "            plot_metrics(voice_results)\n",
    "            \n",
    "            # Plot error metrics\n",
    "            plot_error_and_r2(zero_shot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare zero-shot vs few-shot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison dictionary from the experiment results\n",
    "if \"experiment_results.json\" in results:\n",
    "    experiment_results = results[\"experiment_results.json\"]\n",
    "    \n",
    "    # Check if both zero-shot and few-shot results are available\n",
    "    if \"zero_shot\" in experiment_results and \"few_shot\" in experiment_results:\n",
    "        if \"all_voices\" in experiment_results[\"zero_shot\"] and \"all_voices\" in experiment_results[\"few_shot\"]:\n",
    "            # Create a comparison dictionary\n",
    "            comparison = {\n",
    "                \"evaluations\": [\n",
    "                    experiment_results[\"zero_shot\"][\"all_voices\"],\n",
    "                    experiment_results[\"few_shot\"][\"all_voices\"]\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Plot comparison\n",
    "            compare_models(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from all model directories\n",
    "models_to_compare = [\"mistral\", \"llama3\", \"qwen3:8b\", \"granite3.3\", \"deepseek-r1:32b\", \"qwen3:32b\", \"qwen2.5:7b\"]\n",
    "all_model_results = load_all_model_results(models=models_to_compare, results_dir=\"results_100\")\n",
    "\n",
    "# Compare zero-shot across different models\n",
    "print(\"Comparing zero-shot performance across model architectures:\")\n",
    "zero_shot_comparison = compare_model_architectures(\n",
    "    all_model_results, \n",
    "    mode=\"zero_shot\",\n",
    "    save_path=os.path.join(viz_dir, \"zero_shot_model_comparison.png\")\n",
    ")\n",
    "\n",
    "# Compare few-shot across different models\n",
    "print(\"Comparing few-shot performance across model architectures:\")\n",
    "few_shot_comparison = compare_model_architectures(\n",
    "    all_model_results, \n",
    "    mode=\"few_shot\",\n",
    "    save_path=os.path.join(viz_dir, \"few_shot_model_comparison.png\")\n",
    ")\n",
    "\n",
    "# Compare all approaches across different models\n",
    "print(\"Comparing all approaches across model architectures:\")\n",
    "all_comparison = compare_model_architectures(\n",
    "    all_model_results, \n",
    "    mode=\"both\",\n",
    "    save_path=os.path.join(viz_dir, \"all_models_comparison.png\")\n",
    ")\n",
    "\n",
    "print(\"Creating grouped model comparison (zero-shot and few-shot side by side):\")\n",
    "compare_models_grouped(\n",
    "    all_model_results,\n",
    "    save_path=os.path.join(viz_dir, \"grouped_model_comparison\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scientific_table(all_model_results, output_format=\"latex\"):\n",
    "    \"\"\"\n",
    "    Generate a clean, scientific table from model results.\n",
    "    \n",
    "    Args:\n",
    "        all_model_results: Dictionary of results loaded with load_all_model_results\n",
    "        output_format: \"latex\", \"markdown\", or \"csv\"\n",
    "        \n",
    "    Returns:\n",
    "        Formatted table string\n",
    "    \"\"\"\n",
    "    # Extract metrics for each model and approach\n",
    "    table_data = []\n",
    "    \n",
    "    for model_name, results in all_model_results.items():\n",
    "        # Process zero-shot results\n",
    "        zero_shot_file = next((f for f in results if f == f\"{model_name}_zero_shot.json\"), None)\n",
    "        if zero_shot_file:\n",
    "            metrics = results[zero_shot_file][\"metrics\"]\n",
    "            \n",
    "            # Calculate SSML similarity if not already present\n",
    "            ssml_similarity = 0.0\n",
    "            if \"results\" in results[zero_shot_file]:\n",
    "                try:\n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "                    from sklearn.metrics.pairwise import cosine_similarity\n",
    "                    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                    \n",
    "                    pred_ssmls = []\n",
    "                    gold_ssmls = []\n",
    "                    \n",
    "                    for result in results[zero_shot_file][\"results\"]:\n",
    "                        pred_ssml = result.get(\"predicted_ssml\") or result.get(\"ssml\", \"\")\n",
    "                        gold_ssml = result.get(\"gold_ssml\", \"\")\n",
    "                        \n",
    "                        if pred_ssml.strip() and gold_ssml.strip():\n",
    "                            pred_ssmls.append(pred_ssml)\n",
    "                            gold_ssmls.append(gold_ssml)\n",
    "                    \n",
    "                    if pred_ssmls:\n",
    "                        pred_embeddings = model.encode(pred_ssmls)\n",
    "                        gold_embeddings = model.encode(gold_ssmls)\n",
    "                        \n",
    "                        similarities = []\n",
    "                        for pred_emb, gold_emb in zip(pred_embeddings, gold_embeddings):\n",
    "                            similarity = cosine_similarity([pred_emb], [gold_emb])[0, 0]\n",
    "                            similarities.append(similarity)\n",
    "                        \n",
    "                        ssml_similarity = np.mean(similarities)\n",
    "                except ImportError:\n",
    "                    pass\n",
    "            \n",
    "            # Extract metrics including MSE\n",
    "            row = {\n",
    "                \"Model\": model_name,\n",
    "                \"Approach\": \"Zero-shot\",\n",
    "                \"SSML Similarity\": ssml_similarity,\n",
    "                \"Break MAE\": metrics.get(\"break_time_mae\", 0),\n",
    "                \"Break MSE\": metrics.get(\"break_time_mse\", 0),\n",
    "                \"Pitch MAE\": metrics.get(\"pitch_mae\", 0),\n",
    "                \"Pitch MSE\": metrics.get(\"pitch_mse\", 0),\n",
    "                \"Rate MAE\": metrics.get(\"rate_mae\", 0),\n",
    "                \"Rate MSE\": metrics.get(\"rate_mse\", 0),\n",
    "                \"Volume MAE\": metrics.get(\"volume_mae\", 0),\n",
    "                \"Volume MSE\": metrics.get(\"volume_mse\", 0)\n",
    "            }\n",
    "            table_data.append(row)\n",
    "        \n",
    "        # Process few-shot results\n",
    "        few_shot_file = next((f for f in results if f == f\"{model_name}_few_shot.json\"), None)\n",
    "        if few_shot_file:\n",
    "            metrics = results[few_shot_file][\"metrics\"]\n",
    "            \n",
    "            # Calculate SSML similarity if not already present\n",
    "            ssml_similarity = 0.0\n",
    "            if \"results\" in results[few_shot_file]:\n",
    "                try:\n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "                    from sklearn.metrics.pairwise import cosine_similarity\n",
    "                    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                    \n",
    "                    pred_ssmls = []\n",
    "                    gold_ssmls = []\n",
    "                    \n",
    "                    for result in results[few_shot_file][\"results\"]:\n",
    "                        pred_ssml = result.get(\"predicted_ssml\") or result.get(\"ssml\", \"\")\n",
    "                        gold_ssml = result.get(\"gold_ssml\", \"\")\n",
    "                        \n",
    "                        if pred_ssml.strip() and gold_ssml.strip():\n",
    "                            pred_ssmls.append(pred_ssml)\n",
    "                            gold_ssmls.append(gold_ssml)\n",
    "                    \n",
    "                    if pred_ssmls:\n",
    "                        pred_embeddings = model.encode(pred_ssmls)\n",
    "                        gold_embeddings = model.encode(gold_ssmls)\n",
    "                        \n",
    "                        similarities = []\n",
    "                        for pred_emb, gold_emb in zip(pred_embeddings, gold_embeddings):\n",
    "                            similarity = cosine_similarity([pred_emb], [gold_emb])[0, 0]\n",
    "                            similarities.append(similarity)\n",
    "                        \n",
    "                        ssml_similarity = np.mean(similarities)\n",
    "                except ImportError:\n",
    "                    pass\n",
    "            \n",
    "            # Extract metrics including MSE\n",
    "            row = {\n",
    "                \"Model\": model_name,\n",
    "                \"Approach\": \"Few-shot\",\n",
    "                \"SSML Similarity\": ssml_similarity,\n",
    "                \"Break MAE\": metrics.get(\"break_time_mae\", 0),\n",
    "                \"Break MSE\": metrics.get(\"break_time_mse\", 0),\n",
    "                \"Pitch MAE\": metrics.get(\"pitch_mae\", 0),\n",
    "                \"Pitch MSE\": metrics.get(\"pitch_mse\", 0),\n",
    "                \"Rate MAE\": metrics.get(\"rate_mae\", 0),\n",
    "                \"Rate MSE\": metrics.get(\"rate_mse\", 0),\n",
    "                \"Volume MAE\": metrics.get(\"volume_mae\", 0),\n",
    "                \"Volume MSE\": metrics.get(\"volume_mse\", 0)\n",
    "            }\n",
    "            table_data.append(row)\n",
    "    \n",
    "    # Sort by SSML Similarity descending\n",
    "    table_data.sort(key=lambda x: (-x[\"SSML Similarity\"], x[\"Model\"], x[\"Approach\"]))\n",
    "    \n",
    "    # Generate formatted table based on output format\n",
    "    if output_format == \"latex\":\n",
    "        return _generate_latex_table(table_data)\n",
    "    elif output_format == \"markdown\":\n",
    "        return _generate_markdown_table(table_data)\n",
    "    elif output_format == \"csv\":\n",
    "        return _generate_csv_table(table_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output format: {output_format}\")\n",
    "\n",
    "def _generate_latex_table(table_data):\n",
    "    \"\"\"Generate LaTeX table format with highlighted best values and nicely formatted model names.\"\"\"\n",
    "    # Note: This requires \\usepackage{makecell} and \\usepackage{threeparttable} in your LaTeX preamble\n",
    "    header = (\n",
    "        \"\\\\begin{table*}[htbp!]\\n\"\n",
    "        \"\\\\centering\\n\"\n",
    "        \"\\\\begin{threeparttable}\\n\"\n",
    "        \"\\\\caption{SSML generation performance across models and prompting strategies. Qwen2.5 (7B) offers the best balance of accuracy and efficiency.}\\n\"\n",
    "        \"\\\\label{tab:ssml_performance}\\n\"\n",
    "        \"\\\\begin{tabular}{l@{\\\\hspace{8pt}}c@{\\\\hspace{8pt}}c@{\\\\hspace{8pt}}c@{\\\\hspace{8pt}}c@{\\\\hspace{8pt}}c@{\\\\hspace{8pt}}c}\\n\"\n",
    "        \"\\\\toprule\\n\"\n",
    "        \"Model & \\\\makecell{SSML \\\\\\\\ Sim. $\\\\uparrow$} & \\\\makecell{Pitch \\\\\\\\ MAE/RMSE $\\\\downarrow$} & \"\n",
    "        \"\\\\makecell{Volume \\\\\\\\ MAE/RMSE $\\\\downarrow$} & \\\\makecell{Rate \\\\\\\\ MAE/RMSE $\\\\downarrow$} & \\\\makecell{Break Time \\\\\\\\ MAE/RMSE $\\\\downarrow$} \\\\\\\\ \\n\"\n",
    "        \"\\\\midrule\\n\"\n",
    "    )\n",
    "\n",
    "    # Find best values for MAE (we'll use MAE for highlighting since it's the primary metric)\n",
    "    best = {\n",
    "        \"SSML Similarity\": max(table_data, key=lambda x: x[\"SSML Similarity\"])[\"SSML Similarity\"] if table_data else None,\n",
    "        \"Pitch MAE\": min(table_data, key=lambda x: x[\"Pitch MAE\"])[\"Pitch MAE\"] if table_data else None,\n",
    "        \"Volume MAE\": min(table_data, key=lambda x: x[\"Volume MAE\"])[\"Volume MAE\"] if table_data else None,\n",
    "        \"Rate MAE\": min(table_data, key=lambda x: x[\"Rate MAE\"])[\"Rate MAE\"] if table_data else None,\n",
    "        \"Break MAE\": min(table_data, key=lambda x: x[\"Break MAE\"])[\"Break MAE\"] if table_data else None,\n",
    "    }\n",
    "\n",
    "    def highlight(val, best_val, is_max):\n",
    "        tol = 1e-6\n",
    "        if (is_max and abs(val - best_val) < tol) or (not is_max and abs(val - best_val) < tol):\n",
    "            return \"\\\\cellcolor[gray]{0.9}\"\n",
    "        return \"\"\n",
    "    \n",
    "    def format_model_name(name):\n",
    "        \"\"\"Format model names in a more readable way.\"\"\"\n",
    "        # Handle Qwen models with size indicators\n",
    "        if \"qwen\" in name.lower():\n",
    "            if \":\" in name:\n",
    "                base_name, size = name.split(\":\")\n",
    "                # Capitalize first letter of base name\n",
    "                base_name = base_name[0].upper() + base_name[1:]\n",
    "                # Make size uppercase and add parentheses\n",
    "                size = size.upper()\n",
    "                return f\"{base_name} ({size})\"\n",
    "            else:\n",
    "                # Just capitalize if no size indicator\n",
    "                return name[0].upper() + name[1:]\n",
    "        \n",
    "        # Handle DeepSeek\n",
    "        elif \"deepseek\" in name.lower():\n",
    "            if \":\" in name:\n",
    "                base_name, size = name.split(\":\")\n",
    "                base_name = base_name.replace(\"-\", \"-\")  # Keep dash\n",
    "                # Capitalize first letter and after dash\n",
    "                parts = base_name.split(\"-\")\n",
    "                base_name = \"-\".join([p.capitalize() for p in parts])\n",
    "                size = size.upper()\n",
    "                return f\"{base_name} ({size})\"\n",
    "            else:\n",
    "                parts = name.split(\"-\")\n",
    "                return \"-\".join([p.capitalize() for p in parts])\n",
    "        \n",
    "        # Handle Granite with version\n",
    "        elif \"granite\" in name.lower():\n",
    "            if \".\" in name:\n",
    "                base_name, version = name.split(\".\")\n",
    "                version = \".\" + version  # Keep the dot in version\n",
    "                return f\"{base_name.capitalize()} {version}\"\n",
    "            else:\n",
    "                return name.capitalize()\n",
    "        \n",
    "        # For simple names like \"mistral\" or \"llama3\"\n",
    "        else:\n",
    "            return name.capitalize()\n",
    "    \n",
    "    def format_approach(approach):\n",
    "        \"\"\"Convert approach to short form.\"\"\"\n",
    "        if approach == \"Zero-shot\":\n",
    "            return \"ZS\"\n",
    "        elif approach == \"Few-shot\":\n",
    "            return \"FS\"\n",
    "        else:\n",
    "            return approach\n",
    "\n",
    "    rows = []\n",
    "    for row in table_data:\n",
    "        raw_model_name = row[\"Model\"]\n",
    "        formatted_model_name = format_model_name(raw_model_name)\n",
    "        approach_short = format_approach(row[\"Approach\"])\n",
    "        \n",
    "        # Combine model name and approach\n",
    "        model_with_approach = f\"{formatted_model_name} ({approach_short})\"\n",
    "\n",
    "        ssml_sim = f\"{row['SSML Similarity']:.2f}\"\n",
    "        # Calculate RMSE from MSE\n",
    "        pitch_rmse = (row['Pitch MSE'] ** 0.5) if row['Pitch MSE'] > 0 else 0\n",
    "        volume_rmse = (row['Volume MSE'] ** 0.5) if row['Volume MSE'] > 0 else 0\n",
    "        rate_rmse = (row['Rate MSE'] ** 0.5) if row['Rate MSE'] > 0 else 0\n",
    "        break_rmse = (row['Break MSE'] ** 0.5) if row['Break MSE'] > 0 else 0\n",
    "        \n",
    "        pitch_mae_rmse = f\"{row['Pitch MAE']:.2f}/{pitch_rmse:.2f}\"\n",
    "        volume_mae_rmse = f\"{row['Volume MAE']:.2f}/{volume_rmse:.2f}\"\n",
    "        rate_mae_rmse = f\"{row['Rate MAE']:.2f}/{rate_rmse:.2f}\"\n",
    "        break_mae_rmse = f\"{row['Break MAE']:.2f}/{break_rmse:.2f}\"\n",
    "\n",
    "        ssml_sim_cell = highlight(row[\"SSML Similarity\"], best[\"SSML Similarity\"], is_max=True)\n",
    "        pitch_mae_cell = highlight(row[\"Pitch MAE\"], best[\"Pitch MAE\"], is_max=False)\n",
    "        volume_mae_cell = highlight(row[\"Volume MAE\"], best[\"Volume MAE\"], is_max=False)\n",
    "        rate_mae_cell = highlight(row[\"Rate MAE\"], best[\"Rate MAE\"], is_max=False)\n",
    "        break_mae_cell = highlight(row[\"Break MAE\"], best[\"Break MAE\"], is_max=False)\n",
    "\n",
    "        # Reorder columns: SSML, Pitch, Volume, Rate, Break Time\n",
    "        rows.append(\n",
    "            f\"{model_with_approach} & {ssml_sim_cell}{ssml_sim} & {pitch_mae_cell}{pitch_mae_rmse} & {volume_mae_cell}{volume_mae_rmse} & {rate_mae_cell}{rate_mae_rmse} & {break_mae_cell}{break_mae_rmse} \\\\\\\\\"\n",
    "        )\n",
    "\n",
    "    footer = (\n",
    "        \"\\\\bottomrule\\n\"\n",
    "        \"\\\\end{tabular}\\n\"\n",
    "        \"\\\\begin{tablenotes}\\\\footnotesize\\n\"\n",
    "        \"\\\\item $\\\\uparrow$: higher is better, $\\\\downarrow$: lower is better. ZS: Zero-Shot, FS: Few-Shot.\\n\"\n",
    "        \"\\\\end{tablenotes}\\n\"\n",
    "        \"\\\\end{threeparttable}\\n\"\n",
    "        \"\\\\end{table*}\\n\"\n",
    "    )\n",
    "\n",
    "    return header + \"\\n\".join(rows) + \"\\n\" + footer\n",
    "\n",
    "def _generate_markdown_table(table_data):\n",
    "    \"\"\"Generate Markdown table format.\"\"\"\n",
    "    header = \"| Model | Approach | SSML Sim. ↑ | Pitch MAE ↓ | Volume MAE ↓ | Rate MAE ↓ | Break Time MAE ↓ |\\n\"\n",
    "    header += \"|-------|----------|-------------|-------------|-------------|------------|------------------|\\n\"\n",
    "    \n",
    "    rows = []\n",
    "    for row in table_data:\n",
    "        model_name = row[\"Model\"]\n",
    "        approach = row[\"Approach\"]\n",
    "        ssml_sim = f\"{row['SSML Similarity']:.3f}\"\n",
    "        pitch_mae = f\"{row['Pitch MAE']:.2f}\"\n",
    "        volume_mae = f\"{row['Volume MAE']:.2f}\"\n",
    "        rate_mae = f\"{row['Rate MAE']:.2f}\"\n",
    "        break_mae = f\"{row['Break MAE']:.2f}\"\n",
    "        \n",
    "        # Reorder columns: SSML, Pitch, Volume, Rate, Break Time\n",
    "        rows.append(f\"| {model_name} | {approach} | {ssml_sim} | {pitch_mae} | {volume_mae} | {rate_mae} | {break_mae} |\")\n",
    "    \n",
    "    return header + \"\\n\".join(rows)\n",
    "\n",
    "def _generate_csv_table(table_data):\n",
    "    \"\"\"Generate CSV table format.\"\"\"\n",
    "    header = \"Model,Approach,SSML Similarity,Pitch MAE,Volume MAE,Rate MAE,Break Time MAE\\n\"\n",
    "    \n",
    "    rows = []\n",
    "    for row in table_data:\n",
    "        model_name = row[\"Model\"]\n",
    "        approach = row[\"Approach\"]\n",
    "        ssml_sim = f\"{row['SSML Similarity']:.3f}\"\n",
    "        pitch_mae = f\"{row['Pitch MAE']:.2f}\"\n",
    "        volume_mae = f\"{row['Volume MAE']:.2f}\"\n",
    "        rate_mae = f\"{row['Rate MAE']:.2f}\"\n",
    "        break_mae = f\"{row['Break MAE']:.2f}\"\n",
    "        \n",
    "        # Reorder columns: SSML, Pitch, Volume, Rate, Break Time\n",
    "        rows.append(f\"{model_name},{approach},{ssml_sim},{pitch_mae},{volume_mae},{rate_mae},{break_mae}\")\n",
    "    \n",
    "    return header + \"\\n\".join(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = generate_scientific_table(all_model_results, output_format=\"latex\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scientific_tag_plots(all_model_results, save_path=None, figsize=(10, 3), dpi=500):\n",
    "    \"\"\"\n",
    "    Generate two scientific plots for prosody and break tags, showing model predictions vs gold standard.\n",
    "    Models are ordered by the *best* (lowest absolute error to gold) of zero-shot or few-shot for each tag.\n",
    "    If save_path is given, saves two separate figures: save_path + \"_prosody.png\"/\".pdf\" and save_path + \"_break.png\"/\".pdf\"\n",
    "    \"\"\"\n",
    "    # Set scientific style with larger fonts\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Times New Roman',\n",
    "        'font.serif': ['Computer Modern Roman'],\n",
    "        'font.size': 14,              # Increased from 12\n",
    "        'axes.labelsize': 13,         # Increased from 11\n",
    "        'axes.titlesize': 14,         # Increased from 12\n",
    "        'xtick.labelsize': 11,        # Increased from 9\n",
    "        'ytick.labelsize': 11,        # Increased from 9\n",
    "        'legend.fontsize': 11,        # Increased from 9\n",
    "        'figure.titlesize': 15        # Increased from 13\n",
    "    })\n",
    "\n",
    "    # Format model names nicely\n",
    "    def format_model_name(name):\n",
    "        \"\"\"Format model names in a more readable way.\"\"\"\n",
    "        # Handle Qwen models with size indicators\n",
    "        if \"qwen\" in name.lower():\n",
    "            if \":\" in name:\n",
    "                base_name, size = name.split(\":\")\n",
    "                # Capitalize first letter of base name\n",
    "                base_name = base_name[0].upper() + base_name[1:]\n",
    "                # Make size uppercase and add parentheses\n",
    "                return f\"{base_name}\\n({size.upper()})\"\n",
    "            else:\n",
    "                # Just capitalize if no size indicator\n",
    "                return name[0].upper() + name[1:]\n",
    "        \n",
    "        # Handle DeepSeek\n",
    "        elif \"deepseek\" in name.lower():\n",
    "            if \":\" in name:\n",
    "                base_name, size = name.split(\":\")\n",
    "                return f\"DeeepSeek-R1\\n({size.upper()})\"\n",
    "            else:\n",
    "                parts = name.split(\"-\")\n",
    "                return \"-\".join([p.capitalize() for p in parts])\n",
    "        \n",
    "        # Handle Granite with version\n",
    "        elif \"granite\" in name.lower():\n",
    "            return name.capitalize() + \"\\n(8B)\"\n",
    "        \n",
    "        # For simple names like \"mistral\" or \"llama3\"\n",
    "        elif \"mistral\" in name.lower():\n",
    "            return name.capitalize() + \"\\n(7B)\"\n",
    "        elif \"llama3\" in name.lower():\n",
    "            return name.capitalize() + \"\\n(8B)\"\n",
    "        else:\n",
    "            return name.capitalize()\n",
    "\n",
    "    # Extract data\n",
    "    models = list(all_model_results.keys())\n",
    "    model_data = {}\n",
    "\n",
    "    # Find global gold standard values\n",
    "    gold_prosody_tags = []\n",
    "    gold_break_tags = []\n",
    "\n",
    "    for model in models:\n",
    "        results = all_model_results[model]\n",
    "        model_data[model] = {\n",
    "            'zero_shot': {'prosody': 0, 'break': 0},\n",
    "            'few_shot': {'prosody': 0, 'break': 0},\n",
    "            'gold_prosody': 0,\n",
    "            'gold_break': 0\n",
    "        }\n",
    "\n",
    "        # Get zero-shot results\n",
    "        zero_shot_file = next((f for f in results if f == f\"{model}_zero_shot.json\"), None)\n",
    "        if zero_shot_file:\n",
    "            metrics = results[zero_shot_file][\"metrics\"]\n",
    "            model_data[model]['zero_shot']['prosody'] = metrics.get(\"prosody_count_mean\", 0)\n",
    "            model_data[model]['zero_shot']['break'] = metrics.get(\"break_count_mean\", 0)\n",
    "            model_data[model]['gold_prosody'] = metrics.get(\"gold_prosody_count_mean\", 0)\n",
    "            model_data[model]['gold_break'] = metrics.get(\"gold_break_count_mean\", 0)\n",
    "            gold_prosody_tags.append(metrics.get(\"gold_prosody_count_mean\", 0))\n",
    "            gold_break_tags.append(metrics.get(\"gold_break_count_mean\", 0))\n",
    "\n",
    "        # Get few-shot results\n",
    "        few_shot_file = next((f for f in results if f == f\"{model}_few_shot.json\"), None)\n",
    "        if few_shot_file:\n",
    "            metrics = results[few_shot_file][\"metrics\"]\n",
    "            model_data[model]['few_shot']['prosody'] = metrics.get(\"prosody_count_mean\", 0)\n",
    "            model_data[model]['few_shot']['break'] = metrics.get(\"break_count_mean\", 0)\n",
    "            if model_data[model]['gold_prosody'] == 0:\n",
    "                model_data[model]['gold_prosody'] = metrics.get(\"gold_prosody_count_mean\", 0)\n",
    "                gold_prosody_tags.append(metrics.get(\"gold_prosody_count_mean\", 0))\n",
    "            if model_data[model]['gold_break'] == 0:\n",
    "                model_data[model]['gold_break'] = metrics.get(\"gold_break_count_mean\", 0)\n",
    "                gold_break_tags.append(metrics.get(\"gold_break_count_mean\", 0))\n",
    "\n",
    "    # Calculate average gold standard values\n",
    "    avg_gold_prosody = np.mean(gold_prosody_tags) if gold_prosody_tags else 0\n",
    "    avg_gold_break = np.mean(gold_break_tags) if gold_break_tags else 0\n",
    "\n",
    "    # --- ORDERING: sort by best (closest to gold) value among zero/few-shot for each tag ---\n",
    "\n",
    "    # For prosody tags\n",
    "    prosody_order = []\n",
    "    for model in models:\n",
    "        gold = model_data[model]['gold_prosody']\n",
    "        zero = model_data[model]['zero_shot']['prosody']\n",
    "        few = model_data[model]['few_shot']['prosody']\n",
    "        best_err = min(abs(zero - gold), abs(few - gold))\n",
    "        prosody_order.append((model, best_err))\n",
    "    prosody_order.sort(key=lambda x: x[1])\n",
    "    prosody_sorted = [m[0] for m in prosody_order]\n",
    "\n",
    "    # For break tags\n",
    "    break_order = []\n",
    "    for model in models:\n",
    "        gold = model_data[model]['gold_break']\n",
    "        zero = model_data[model]['zero_shot']['break']\n",
    "        few = model_data[model]['few_shot']['break']\n",
    "        best_err = min(abs(zero - gold), abs(few - gold))\n",
    "        break_order.append((model, best_err))\n",
    "    break_order.sort(key=lambda x: x[1])\n",
    "    break_sorted = [m[0] for m in break_order]\n",
    "\n",
    "    # --- Plot 1: Prosody Tags ---\n",
    "    # Increase figure size slightly for better readability\n",
    "    fig1, ax1 = plt.subplots(figsize=(figsize[0]//2 + 1, figsize[1] + 1))\n",
    "    x1 = np.arange(len(prosody_sorted))\n",
    "    zero_shot_prosody = [model_data[model]['zero_shot']['prosody'] for model in prosody_sorted]\n",
    "    few_shot_prosody = [model_data[model]['few_shot']['prosody'] for model in prosody_sorted]\n",
    "    ax1.bar(x1 - 0.175, zero_shot_prosody, 0.35, label='Zero-Shot', color='#4477AA', alpha=0.8)\n",
    "    ax1.bar(x1 + 0.175, few_shot_prosody, 0.35, label='Few-Shot', color='#EE6677', alpha=0.8)\n",
    "    ax1.axhline(y=avg_gold_prosody, color='black', linestyle='--', linewidth=1.5, label='Gold Standard')\n",
    "    # ax1.set_title('Prosody Tags per Sample', pad=10)\n",
    "    ax1.set_ylabel('Average Count')\n",
    "    ax1.set_xticks(x1)\n",
    "    # Use the format_model_name function for better labels\n",
    "    ax1.set_xticklabels([format_model_name(name) for name in prosody_sorted], rotation=45, ha='right')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    ax1.legend(frameon=True, fancybox=False, edgecolor='black')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    # --- Plot 2: Break Tags ---\n",
    "    # Increase figure size slightly for better readability\n",
    "    fig2, ax2 = plt.subplots(figsize=(figsize[0]//2 + 1, figsize[1] + 1))\n",
    "    x2 = np.arange(len(break_sorted))\n",
    "    zero_shot_break = [model_data[model]['zero_shot']['break'] for model in break_sorted]\n",
    "    few_shot_break = [model_data[model]['few_shot']['break'] for model in break_sorted]\n",
    "    ax2.bar(x2 - 0.175, zero_shot_break, 0.35, label='Zero-Shot', color='#4477AA', alpha=0.8)\n",
    "    ax2.bar(x2 + 0.175, few_shot_break, 0.35, label='Few-Shot', color='#EE6677', alpha=0.8)\n",
    "    ax2.axhline(y=avg_gold_break, color='black', linestyle='--', linewidth=1.5, label='Gold Standard')\n",
    "    # ax2.set_title('Break Tags per Sample', pad=10)\n",
    "    ax2.set_ylabel('Average Count')\n",
    "    ax2.set_xticks(x2)\n",
    "    # Use the format_model_name function for better labels\n",
    "    ax2.set_xticklabels([format_model_name(name) for name in break_sorted], rotation=45, ha='right')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    ax2.legend(frameon=True, fancybox=False, edgecolor='black')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        fig1.savefig(f\"{save_path}_prosody.pdf\", format='pdf', bbox_inches='tight', dpi=dpi, pad_inches=0)\n",
    "        fig1.savefig(f\"{save_path}_prosody.png\", format='png', bbox_inches='tight', dpi=dpi, pad_inches=0)\n",
    "        fig2.savefig(f\"{save_path}_break.pdf\", format='pdf', bbox_inches='tight', dpi=dpi, pad_inches=0)\n",
    "        fig2.savefig(f\"{save_path}_break.png\", format='png', bbox_inches='tight', dpi=dpi, pad_inches=0)\n",
    "\n",
    "    return fig1, fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Tell matplotlib to rebuild its font cache\n",
    "font_path = \"/home/infres/horstmann-24/.local/share/fonts/truetype/msttcorefonts/times.ttf\"\n",
    "fm.fontManager.addfont(font_path)\n",
    "# Now list all fonts to verify it's included\n",
    "for f in fm.findSystemFonts(fontpaths=['/home/infres/horstmann-24/.local/share/fonts']):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, fig2 = generate_scientific_tag_plots(all_model_results, save_path=\"ssml_tag_usage_comparison\")\n",
    "# Or to display them in the notebook\n",
    "fig1, fig2 = generate_scientific_tag_plots(all_model_results)\n",
    "\n",
    "# Display the generated figures\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analysis\n",
    "\n",
    "Here you can add custom analysis code to examine specific aspects of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_break_position_accuracy(results_data):\n",
    "    \"\"\"Analyze how the break position threshold affects accuracy\"\"\"\n",
    "    # This is a placeholder for a more detailed analysis\n",
    "    # You could implement this to test different thresholds\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze specific examples to understand why breaks might be missed or falsely detected\n",
    "def analyze_break_errors(results_data):\n",
    "    if \"results\" not in results_data:\n",
    "        print(\"No detailed results found\")\n",
    "        return\n",
    "    \n",
    "    # Find samples with parsed_sequence\n",
    "    for i, result in enumerate(results_data[\"results\"]):\n",
    "        if \"params\" in result and \"parsed_sequence\" in result[\"params\"]:\n",
    "            print(f\"Sample {i+1}: {result['input_text'][:50]}...\")\n",
    "            pred_seq = result[\"params\"][\"parsed_sequence\"]\n",
    "            \n",
    "            # Print break positions\n",
    "            breaks = []\n",
    "            for j, item in enumerate(pred_seq):\n",
    "                if item.get(\"type\") == \"break\":\n",
    "                    breaks.append(f\"Position {j}: {item.get('time', '?')}\")\n",
    "            \n",
    "            print(f\"Found {len(breaks)} breaks:\")\n",
    "            for b in breaks:\n",
    "                print(f\"  - {b}\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze break errors in zero-shot results\n",
    "if \"zero_shot_mistral.json\" in results:\n",
    "    analyze_break_errors(results[\"zero_shot_mistral.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Updated Visualizations\n",
    "\n",
    "Save any visualizations to disk if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save visualizations\n",
    "\n",
    "# Example: Save zero-shot visualizations\n",
    "if \"zero_shot_mistral.json\" in results:\n",
    "    zero_shot_results = results[\"zero_shot_mistral.json\"]\n",
    "    plot_metrics(zero_shot_results, save_path=os.path.join(viz_dir, \"zero_shot_metrics.png\"))\n",
    "    plot_error_metrics(zero_shot_results, save_path=os.path.join(viz_dir, \"zero_shot_errors.png\"))\n",
    "\n",
    "print(f\"Visualizations saved to {viz_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
